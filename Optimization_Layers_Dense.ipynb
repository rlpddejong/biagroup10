{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of the Layers\n",
    "Course: 8P361  &emsp;&emsp;  Group: 10\n",
    "\n",
    "Helgers, V.M.J. \t&emsp;&emsp; 1329332 <br>\n",
    "Jong, de R.L.P.D. \t&emsp; 1328328 <br>\n",
    "Korsten, T.\t\t    &emsp;&emsp;&emsp;&emsp; 1340522 <br>\n",
    "Moharir, S. \t\t&emsp;&emsp;&emsp;&emsp; 1296256 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes\n",
    "\n",
    "1. Note that all functions needed to execute this notebook can be found in the Optimization_Layers.py file.\n",
    "\n",
    "2. If one wants to run this notebook it is important to add the right path to the image data within the function 'get_pcam_generators' which is called below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First, import the relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimization_Layers import get_pcam_generators, get_model, train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, there will be an optimization for the layers in the model. There will be optimized with the number of blocks consisting of convolutional layers and a max pooling layer, which will varie between 2 and 3 blocks since there is no possibility of more max pooling layers with the input format used. Furthermore there will be optimized with the number of convolutional layers per block, consisting with convolutional layers and a max pooling layer. This number is for now set to vary between 1 and 3. Too many layers may lead to overfitting of the model, so therefore this choice has been made. Furthermore for every combination of the blocks and convolutional layers, 3 models are created with a different first dense layer, e.g. Dense(x, activation ='relu'), with x from [64,128,264]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before every model is run, the dataset is loaded in. This is done again inbefore running a new model, for the sake of keeping the circumstances the same for every model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Model with 2 blocks, containing 1 convolutional layer per block, densenumber equal to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen2164, val_gen2164 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/8p361-lecturer/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 126s 27ms/step - loss: 0.5141 - accuracy: 0.7464 - val_loss: 0.4260 - val_accuracy: 0.8073\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42601, saving model to model2164_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.4089 - accuracy: 0.8149 - val_loss: 0.3668 - val_accuracy: 0.8373\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42601 to 0.36685, saving model to model2164_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3523 - accuracy: 0.8464 - val_loss: 0.3401 - val_accuracy: 0.8485\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36685 to 0.34014, saving model to model2164_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model2164 = get_model(2,1,64)\n",
    "\n",
    "# Train the model\n",
    "train_model(model2164, train_gen2164, val_gen2164, 'model2164')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model with 2 blocks, containing 1 convolutional layer per block, densenumber equal to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen21128, val_gen21128 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 28ms/step - loss: 0.5274 - accuracy: 0.7317 - val_loss: 0.4326 - val_accuracy: 0.8020\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43256, saving model to model21128_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.4050 - accuracy: 0.8174 - val_loss: 0.3618 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43256 to 0.36178, saving model to model21128_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3624 - accuracy: 0.8417 - val_loss: 0.3364 - val_accuracy: 0.8548\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36178 to 0.33642, saving model to model21128_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model21128 = get_model(2,1,128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model21128, train_gen21128, val_gen21128, 'model21128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Model with 2 blocks, containing 1 convolutional layer per block, densenumber equal to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen21256, val_gen21256 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 125s 28ms/step - loss: 0.5196 - accuracy: 0.7351 - val_loss: 0.4353 - val_accuracy: 0.7981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43528, saving model to model21256_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.4159 - accuracy: 0.8132 - val_loss: 0.3935 - val_accuracy: 0.8244\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43528 to 0.39350, saving model to model21256_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3674 - accuracy: 0.8400 - val_loss: 0.3309 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39350 to 0.33086, saving model to model21256_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model21256 = get_model(2,1,256)\n",
    "\n",
    "# Train the model\n",
    "train_model(model21256, train_gen21256, val_gen21256, 'model21256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Model with 3 blocks, containing 1 convolutional layer per block, densenumber equal to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen3164, val_gen3164 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.5363 - accuracy: 0.7295 - val_loss: 0.4112 - val_accuracy: 0.8173\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41120, saving model to model3164_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 124s 28ms/step - loss: 0.4144 - accuracy: 0.8126 - val_loss: 0.3635 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.41120 to 0.36352, saving model to model3164_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3623 - accuracy: 0.8418 - val_loss: 0.3265 - val_accuracy: 0.8621\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36352 to 0.32655, saving model to model3164_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model3164 = get_model(3,1,64)\n",
    "\n",
    "# Train the model\n",
    "train_model(model3164, train_gen3164, val_gen3164, 'model3164')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Model with 3 blocks, containing 1 convolutional layer per block, densenumber equal to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen31128, val_gen31128 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.5613 - accuracy: 0.7013 - val_loss: 0.4568 - val_accuracy: 0.7841\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45681, saving model to model31128_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.4131 - accuracy: 0.8158 - val_loss: 0.3851 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45681 to 0.38510, saving model to model31128_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3734 - accuracy: 0.8359 - val_loss: 0.3492 - val_accuracy: 0.8483\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38510 to 0.34921, saving model to model31128_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model31128 = get_model(3,1,128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model31128, train_gen31128, val_gen31128, 'model31128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Model with 3 blocks, containing 1 convolutional layer per block, densenumber equal to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen31256, val_gen31256 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 125s 28ms/step - loss: 0.5396 - accuracy: 0.7200 - val_loss: 0.4268 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42683, saving model to model31256_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 124s 28ms/step - loss: 0.4015 - accuracy: 0.8221 - val_loss: 0.3525 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42683 to 0.35249, saving model to model31256_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3600 - accuracy: 0.8441 - val_loss: 0.3553 - val_accuracy: 0.8448\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35249\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model31256 = get_model(3,1,256)\n",
    "\n",
    "# Train the model\n",
    "train_model(model31256, train_gen31256, val_gen31256, 'model31256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Model with 2 blocks, containing 2 convolutional layer per block, densenumber equal to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen2264, val_gen2264 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.6925 - accuracy: 0.5098 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69412, saving model to model2264_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 128s 28ms/step - loss: 0.6919 - accuracy: 0.5113 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69412 to 0.69331, saving model to model2264_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.6069 - accuracy: 0.6335 - val_loss: 0.4138 - val_accuracy: 0.8096\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69331 to 0.41381, saving model to model2264_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model2264 = get_model(2,2,64)\n",
    "\n",
    "# Train the model\n",
    "train_model(model2264, train_gen2264, val_gen2264, 'model2264')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Model with 2 blocks, containing 2 convolutional layer per block, densenumber equal to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen22128, val_gen22128 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.6047 - accuracy: 0.6314 - val_loss: 0.4335 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43350, saving model to model22128_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.4279 - accuracy: 0.8053 - val_loss: 0.3635 - val_accuracy: 0.8391\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43350 to 0.36348, saving model to model22128_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.3503 - accuracy: 0.8474 - val_loss: 0.3333 - val_accuracy: 0.8616\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36348 to 0.33327, saving model to model22128_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model22128 = get_model(2,2,128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model22128, train_gen22128, val_gen22128, 'model22128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 Model with 2 blocks, containing 2 convolutional layer per block, densenumber equal to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen22256, val_gen22256 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.5626 - accuracy: 0.6868 - val_loss: 0.4573 - val_accuracy: 0.7896\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45729, saving model to model22256_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.4284 - accuracy: 0.8050 - val_loss: 0.4030 - val_accuracy: 0.8216\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45729 to 0.40302, saving model to model22256_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.3892 - accuracy: 0.8270 - val_loss: 0.3477 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40302 to 0.34770, saving model to model22256_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model22256 = get_model(2,2,256)\n",
    "\n",
    "# Train the model\n",
    "train_model(model22256, train_gen22256, val_gen22256, 'model22256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10 Model with 3 blocks, containing 2 convolutional layer per block, densenumber equal to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen3264, val_gen3264 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.5714 - accuracy: 0.6796 - val_loss: 0.4537 - val_accuracy: 0.7909\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45367, saving model to model3264_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.4197 - accuracy: 0.8083 - val_loss: 0.3636 - val_accuracy: 0.8377\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45367 to 0.36365, saving model to model3264_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3482 - accuracy: 0.8481 - val_loss: 0.2872 - val_accuracy: 0.8754\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36365 to 0.28721, saving model to model3264_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model3264 = get_model(3,2,64)\n",
    "\n",
    "# Train the model\n",
    "train_model(model3264, train_gen3264, val_gen3264, 'model3264')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.11 Model with 3 blocks, containing 2 convolutional layer per block, densenumber equal to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen32128, val_gen32128 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.6875 - accuracy: 0.5329 - val_loss: 0.6966 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69657, saving model to model32128_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.5232 - accuracy: 0.7367 - val_loss: 0.3849 - val_accuracy: 0.8345\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69657 to 0.38491, saving model to model32128_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3895 - accuracy: 0.8273 - val_loss: 0.3690 - val_accuracy: 0.8334\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38491 to 0.36904, saving model to model32128_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model32128 = get_model(3,2,128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model32128, train_gen32128, val_gen32128, 'model32128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.12 Model with 3 blocks, containing 2 convolutional layer per block, densenumber equal to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen32256, val_gen32256 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.5789 - accuracy: 0.6746 - val_loss: 0.4271 - val_accuracy: 0.8099\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42708, saving model to model32256_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.4008 - accuracy: 0.8207 - val_loss: 0.3091 - val_accuracy: 0.8651\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42708 to 0.30910, saving model to model32256_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.3253 - accuracy: 0.8594 - val_loss: 0.3468 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30910\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model32256 = get_model(3,2,256)\n",
    "\n",
    "# Train the model\n",
    "train_model(model32256, train_gen32256, val_gen32256, 'model32256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.13 Model with 2 blocks, containing 3 convolutional layers per block, densenumber equal to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen2364, val_gen2364 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.6009 - accuracy: 0.6360 - val_loss: 0.4851 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48513, saving model to model2364_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 117s 26ms/step - loss: 0.4259 - accuracy: 0.8058 - val_loss: 0.3837 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48513 to 0.38365, saving model to model2364_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.3776 - accuracy: 0.8316 - val_loss: 0.3731 - val_accuracy: 0.8359\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38365 to 0.37313, saving model to model2364_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model2364 = get_model(2,3,64)\n",
    "\n",
    "# Train the model\n",
    "train_model(model2364, train_gen2364, val_gen2364, 'model2364')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.14 Model with 2 blocks, containing 3 convolutional layers per block, densenumber equal to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen23128, val_gen23128 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.6925 - accuracy: 0.5115 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69339, saving model to model23128_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.6611 - accuracy: 0.5632 - val_loss: 0.4437 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69339 to 0.44367, saving model to model23128_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 123s 27ms/step - loss: 0.4487 - accuracy: 0.7966 - val_loss: 0.4310 - val_accuracy: 0.8043\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44367 to 0.43098, saving model to model23128_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model23128 = get_model(2,3,128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model23128, train_gen23128, val_gen23128, 'model23128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.15 Model with 2 blocks, containing 3 convolutional layers per block, densenumber equal to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen23256, val_gen23256 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 116s 26ms/step - loss: 0.6097 - accuracy: 0.6398 - val_loss: 0.4218 - val_accuracy: 0.8070\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42184, saving model to model23256_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 117s 26ms/step - loss: 0.4256 - accuracy: 0.8056 - val_loss: 0.3760 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42184 to 0.37603, saving model to model23256_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 119s 26ms/step - loss: 0.3539 - accuracy: 0.8444 - val_loss: 0.3012 - val_accuracy: 0.8693\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37603 to 0.30120, saving model to model23256_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model23256 = get_model(2,3,256)\n",
    "\n",
    "# Train the model\n",
    "train_model(model23256, train_gen23256, val_gen23256, 'model23256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.16 Model with 3 blocks, containing 3 convolutional layers per block, densenumber equal to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen3364, val_gen3364 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 118s 26ms/step - loss: 0.6920 - accuracy: 0.5131 - val_loss: 0.6824 - val_accuracy: 0.5567\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68243, saving model to model3364_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 116s 26ms/step - loss: 0.6249 - accuracy: 0.6375 - val_loss: 0.4336 - val_accuracy: 0.8011\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68243 to 0.43364, saving model to model3364_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 115s 26ms/step - loss: 0.4387 - accuracy: 0.8020 - val_loss: 0.4139 - val_accuracy: 0.8022\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43364 to 0.41386, saving model to model3364_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model3364 = get_model(3,3,64)\n",
    "\n",
    "# Train the model\n",
    "train_model(model3364, train_gen3364, val_gen3364, 'model3364')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.17 Model with 3 blocks, containing 3 convolutional layers per block, densenumber equal to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen33128, val_gen33128 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 124s 27ms/step - loss: 0.6543 - accuracy: 0.5885 - val_loss: 0.4535 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45350, saving model to model33128_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 122s 27ms/step - loss: 0.4446 - accuracy: 0.7975 - val_loss: 0.3395 - val_accuracy: 0.8553\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45350 to 0.33946, saving model to model33128_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 121s 27ms/step - loss: 0.3568 - accuracy: 0.8444 - val_loss: 0.3027 - val_accuracy: 0.8754\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33946 to 0.30272, saving model to model33128_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model33128 = get_model(3,3,128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model33128, train_gen33128, val_gen33128, 'model33128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.18 Model with 3 blocks, containing 3 convolutional layers per block, densenumber equal to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen33256, val_gen33256 = get_pcam_generators('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4500/4500 [==============================] - 121s 27ms/step - loss: 0.6904 - accuracy: 0.5182 - val_loss: 0.6244 - val_accuracy: 0.6532\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62436, saving model to model33256_weights.hdf5\n",
      "Epoch 2/3\n",
      "4500/4500 [==============================] - 120s 27ms/step - loss: 0.5753 - accuracy: 0.6898 - val_loss: 0.3849 - val_accuracy: 0.8276\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62436 to 0.38492, saving model to model33256_weights.hdf5\n",
      "Epoch 3/3\n",
      "4500/4500 [==============================] - 119s 27ms/step - loss: 0.3636 - accuracy: 0.8426 - val_loss: 0.2894 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38492 to 0.28936, saving model to model33256_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "model33256 = get_model(3,3,256)\n",
    "\n",
    "# Train the model\n",
    "train_model(model33256, train_gen33256, val_gen33256, 'model33256')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
